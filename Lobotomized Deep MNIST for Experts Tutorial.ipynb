{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [Deep MNIST for Experts Tutorial](https://www.tensorflow.org/versions/r0.10/tutorials/mnist/pros/index.html)\n",
    "\n",
    "This is the result of me going through the Deep MNIST for Experts Tutorial and adding some more explanations to areas I found confusing. I've also turned a lot of magic numbers into variables - both to explain and allow tweaking. I then decided to see what could be done with a smaller network - the tutorial says it may take half an hour to run, but clearly that's on beefier hardware than my mid-2014 13\" Macbook Pro.\n",
    "\n",
    "The original parameters in the tutorial are:\n",
    "- First Convolutional Layer: 32\n",
    "- Second Convolutional Layer: 64\n",
    "- Fully Connected Layer: 1024\n",
    "\n",
    "And is set to run for 20,000 iterations to acheive 99.2% accuracy.\n",
    "\n",
    "I've reduced this to:\n",
    "- First Convolutional Layer: 8\n",
    "- Second Convolutional Layer: 16\n",
    "- Fully Connected Layer: 64\n",
    "\n",
    "For a maximum of 10001 iterations, with some extra stopping conditions when validation accuracy starts to decline, which results in about 98% accuracy. This takes about 35s per 100 training iterations on my laptop, as compared to about 160-170s with the original parameters, and will generally stop around 7000 iterations.\n",
    "\n",
    "Basically, I was able to make the network quite a lot smaller and faster to train, while only making it a little bit dumber. This shows how that last little bit more performance can be quite expensive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Interactive Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a batch look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.38039219,  0.37647063,  0.3019608 ,\n",
       "          0.46274513,  0.2392157 ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.35294119,  0.5411765 ,  0.92156869,\n",
       "          0.92156869,  0.92156869,  0.92156869,  0.92156869,  0.92156869,\n",
       "          0.98431379,  0.98431379,  0.97254908,  0.99607849,  0.96078438,\n",
       "          0.92156869,  0.74509805,  0.08235294,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.54901963,\n",
       "          0.98431379,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "          0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "          0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "          0.74117649,  0.09019608,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.88627458,  0.99607849,  0.81568635,\n",
       "          0.78039223,  0.78039223,  0.78039223,  0.78039223,  0.54509807,\n",
       "          0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,\n",
       "          0.50196081,  0.8705883 ,  0.99607849,  0.99607849,  0.74117649,\n",
       "          0.08235294,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.14901961,  0.32156864,  0.0509804 ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.13333334,\n",
       "          0.83529419,  0.99607849,  0.99607849,  0.45098042,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.32941177,  0.99607849,\n",
       "          0.99607849,  0.91764712,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.32941177,  0.99607849,  0.99607849,  0.91764712,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.41568631,  0.6156863 ,\n",
       "          0.99607849,  0.99607849,  0.95294124,  0.20000002,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.09803922,  0.45882356,  0.89411771,  0.89411771,\n",
       "          0.89411771,  0.99215692,  0.99607849,  0.99607849,  0.99607849,\n",
       "          0.99607849,  0.94117653,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.26666668,  0.4666667 ,  0.86274517,\n",
       "          0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "          0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.55686277,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.14509805,  0.73333335,\n",
       "          0.99215692,  0.99607849,  0.99607849,  0.99607849,  0.87450987,\n",
       "          0.80784321,  0.80784321,  0.29411766,  0.26666668,  0.84313732,\n",
       "          0.99607849,  0.99607849,  0.45882356,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.44313729,  0.8588236 ,  0.99607849,  0.94901967,  0.89019614,\n",
       "          0.45098042,  0.34901962,  0.12156864,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.7843138 ,  0.99607849,  0.9450981 ,\n",
       "          0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.66274512,  0.99607849,\n",
       "          0.6901961 ,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.18823531,\n",
       "          0.90588242,  0.99607849,  0.91764712,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.07058824,  0.48627454,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "          0.65098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.54509807,  0.99607849,  0.9333334 ,  0.22352943,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.82352948,  0.98039222,  0.99607849,\n",
       "          0.65882355,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.94901967,  0.99607849,  0.93725497,  0.22352943,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.34901962,  0.98431379,  0.9450981 ,\n",
       "          0.33725491,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "          0.80784321,  0.96470594,  0.6156863 ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.01568628,  0.45882356,  0.27058825,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ]], dtype=float32),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.next_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**\n",
    "\n",
    "A variable lives in TensorFlow's computation graph, and is used or modified by the computation. Model parameters are typically defined as variables. Variables must be initialized before they are used within a session; can be done for all variables at once with `initialize_all_variables`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this CNN, we want weights with a bit of noise, and we want the bias to be slightly positive to avoid dead ReLU neurons.\n",
    "\n",
    "Truncated normal: random values from a normal distribution, but ensures that all values are within 2 stddev of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions with a size of one, zero padded so the output is the same size.\n",
    "\n",
    "Max pooling over 2x2 blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First convolutional layer:\n",
    "\n",
    "Compute a bunch of features for each 5x5 patch in the image.\n",
    "2x2 max pooling reduces the images to 14x14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1_num_features = 8\n",
    "patch_size = 5\n",
    "\n",
    "W_conv1 = weight_variable([patch_size, patch_size, 1, conv1_num_features])\n",
    "b_conv1 = bias_variable([conv1_num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second convolutional layer:\n",
    "\n",
    "More features for each 5x5 patch of the image, which has now been compressed to 14x14 by pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv2_num_features = 16\n",
    "W_conv2 = weight_variable([5, 5, conv1_num_features, conv2_num_features])\n",
    "b_conv2 = bias_variable([conv2_num_features])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now reduced the image to 7x7 by applying 2x2 pooling again\n",
    "\n",
    "Add a fully connected layer neurons to bring the pieces together and process the entire image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc_layer_size = 64\n",
    "W_fc1 = weight_variable([7*7*conv2_num_features, fc_layer_size])\n",
    "b_fc1 = bias_variable([fc_layer_size])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*conv2_num_features])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readout layer - Softmax\n",
    "Size is 10 because there are 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readout_size = 10\n",
    "W_fc2 = weight_variable([fc_layer_size, readout_size])\n",
    "b_fc2 = bias_variable([readout_size])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & Evaluate the CNN model\n",
    "\n",
    "- We will stop training:\n",
    "    - After 10000 iterations\n",
    "    - When the validation accuracy doesn't improve much for 3 validation cycles\n",
    "    - The validation accuracy drops considerably below the best and doesn't recover for 3 validation cycles (may be combined with previous condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, trainining accuracy 0.562 34.3138 seconds elapsed\n",
      "step 200, trainining accuracy 0.716 67.8487 seconds elapsed\n",
      "step 300, trainining accuracy 0.814 101.361 seconds elapsed\n",
      "step 400, trainining accuracy 0.864 134.265 seconds elapsed\n",
      "step 500, trainining accuracy 0.882 166.92 seconds elapsed\n",
      "**step 500, validation accuracy 0.881 167.303 seconds**\n",
      "step 600, trainining accuracy 0.886 201.842 seconds elapsed\n",
      "step 700, trainining accuracy 0.928 234.789 seconds elapsed\n",
      "step 800, trainining accuracy 0.918 267.706 seconds elapsed\n",
      "step 900, trainining accuracy 0.91 300.732 seconds elapsed\n",
      "step 1000, trainining accuracy 0.914 333.556 seconds elapsed\n",
      "**step 1000, validation accuracy 0.929 333.906 seconds**\n",
      "step 1100, trainining accuracy 0.912 367.366 seconds elapsed\n",
      "step 1200, trainining accuracy 0.932 400.112 seconds elapsed\n",
      "step 1300, trainining accuracy 0.932 432.929 seconds elapsed\n",
      "step 1400, trainining accuracy 0.928 465.69 seconds elapsed\n",
      "step 1500, trainining accuracy 0.966 498.451 seconds elapsed\n",
      "**step 1500, validation accuracy 0.944 498.803 seconds**\n",
      "step 1600, trainining accuracy 0.95 532.092 seconds elapsed\n",
      "step 1700, trainining accuracy 0.94 564.911 seconds elapsed\n",
      "step 1800, trainining accuracy 0.968 597.698 seconds elapsed\n",
      "step 1900, trainining accuracy 0.94 630.485 seconds elapsed\n",
      "step 2000, trainining accuracy 0.968 663.149 seconds elapsed\n",
      "**step 2000, validation accuracy 0.969 663.492 seconds**\n",
      "step 2100, trainining accuracy 0.97 696.919 seconds elapsed\n",
      "step 2200, trainining accuracy 0.95 729.629 seconds elapsed\n",
      "step 2300, trainining accuracy 0.942 762.366 seconds elapsed\n",
      "step 2400, trainining accuracy 0.964 795.112 seconds elapsed\n",
      "step 2500, trainining accuracy 0.96 827.939 seconds elapsed\n",
      "**step 2500, validation accuracy 0.958 828.315 seconds**\n",
      "step 2600, trainining accuracy 0.966 861.098 seconds elapsed\n",
      "step 2700, trainining accuracy 0.962 893.77 seconds elapsed\n",
      "step 2800, trainining accuracy 0.97 926.736 seconds elapsed\n",
      "step 2900, trainining accuracy 0.964 960.035 seconds elapsed\n",
      "step 3000, trainining accuracy 0.95 992.868 seconds elapsed\n",
      "**step 3000, validation accuracy 0.967 993.187 seconds**\n",
      "step 3100, trainining accuracy 0.966 1025.87 seconds elapsed\n",
      "step 3200, trainining accuracy 0.976 1058.56 seconds elapsed\n",
      "step 3300, trainining accuracy 0.968 1091.25 seconds elapsed\n",
      "step 3400, trainining accuracy 0.968 1123.97 seconds elapsed\n",
      "step 3500, trainining accuracy 0.966 1156.9 seconds elapsed\n",
      "**step 3500, validation accuracy 0.974 1157.23 seconds**\n",
      "step 3600, trainining accuracy 0.978 1190.71 seconds elapsed\n",
      "step 3700, trainining accuracy 0.974 1223.49 seconds elapsed\n",
      "step 3800, trainining accuracy 0.974 1256.04 seconds elapsed\n",
      "step 3900, trainining accuracy 0.976 1289 seconds elapsed\n",
      "step 4000, trainining accuracy 0.986 1321.68 seconds elapsed\n",
      "**step 4000, validation accuracy 0.974 1322.01 seconds**\n",
      "step 4100, trainining accuracy 0.966 1354.72 seconds elapsed\n",
      "step 4200, trainining accuracy 0.96 1387.3 seconds elapsed\n",
      "step 4300, trainining accuracy 0.984 1419.99 seconds elapsed\n",
      "step 4400, trainining accuracy 0.976 1452.92 seconds elapsed\n",
      "step 4500, trainining accuracy 0.968 1485.57 seconds elapsed\n",
      "**step 4500, validation accuracy 0.978 1485.91 seconds**\n",
      "step 4600, trainining accuracy 0.97 1519.23 seconds elapsed\n",
      "step 4700, trainining accuracy 0.972 1551.93 seconds elapsed\n",
      "step 4800, trainining accuracy 0.978 1584.8 seconds elapsed\n",
      "step 4900, trainining accuracy 0.98 1617.4 seconds elapsed\n",
      "step 5000, trainining accuracy 0.98 1650.05 seconds elapsed\n",
      "**step 5000, validation accuracy 0.978 1650.37 seconds**\n",
      "step 5100, trainining accuracy 0.984 1683.15 seconds elapsed\n",
      "step 5200, trainining accuracy 0.97 1715.86 seconds elapsed\n",
      "step 5300, trainining accuracy 0.984 1748.55 seconds elapsed\n",
      "step 5400, trainining accuracy 0.966 1781.43 seconds elapsed\n",
      "step 5500, trainining accuracy 0.976 1814.34 seconds elapsed\n",
      "**step 5500, validation accuracy 0.979 1814.68 seconds**\n",
      "step 5600, trainining accuracy 0.97 1847.95 seconds elapsed\n",
      "step 5700, trainining accuracy 0.972 1880.67 seconds elapsed\n",
      "step 5800, trainining accuracy 0.97 1913.39 seconds elapsed\n",
      "step 5900, trainining accuracy 0.988 1946.14 seconds elapsed\n",
      "step 6000, trainining accuracy 0.968 1978.73 seconds elapsed\n",
      "**step 6000, validation accuracy 0.969 1979.05 seconds**\n",
      "test accuracy on last model 0.9796\n",
      "test accuracy on best model 0.9791 mnist-tutorial-5500\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "max_iterations = 10001 # ensure we get the last validation!\n",
    "prev_validation_accuracy = 0\n",
    "max_validation_accuracy = 0\n",
    "\n",
    "# stop if the validation accuracy doesn't improve by 0.001 for 3 iterations\n",
    "validation_difference = 0.001\n",
    "max_validations_with_no_improvement = 3\n",
    "validations_with_no_improvement = 0\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "best_model_path = \"\"\n",
    "start_time = time.time()\n",
    "for i in range(max_iterations):\n",
    "    batch = mnist.train.next_batch(500)\n",
    "\n",
    "    if i%100 == 0 and i != 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "                x:batch[0], y_:batch[1], keep_prob:1.0\n",
    "            })\n",
    "        print(\"step %d, trainining accuracy %g %g seconds elapsed\"%(i, train_accuracy, time.time()-start_time))\n",
    "    if i%500 == 0 and i != 0:\n",
    "        val = mnist.validation.next_batch(1000)\n",
    "        validation_accuracy = accuracy.eval(feed_dict={\n",
    "                x:val[0], y_:val[1], keep_prob:1.0\n",
    "            })\n",
    "        print(\"**step %d, validation accuracy %g %g seconds**\"%(i, validation_accuracy, time.time()-start_time))\n",
    "        if validation_accuracy > max_validation_accuracy:\n",
    "            best_model_path = saver.save(sess, 'mnist-tutorial', global_step=i)\n",
    "            max_validation_accuracy = validation_accuracy\n",
    "        if (validation_accuracy < (prev_validation_accuracy + validation_difference)) \\\n",
    "            or validation_accuracy < max_validation_accuracy - 0.1: # Sometimes weights hit 0 and accuracy plummets\n",
    "            validations_with_no_improvement += 1\n",
    "            if validations_with_no_improvement >= max_validations_with_no_improvement:\n",
    "                break\n",
    "        else:\n",
    "            validations_with_no_improvement = 0\n",
    "        \n",
    "        prev_validation_accuracy = validation_accuracy\n",
    "           \n",
    "    train_step.run(feed_dict={x:batch[0], y_:batch[1], keep_prob:0.5})\n",
    "\n",
    "print(\"test accuracy on last model %g\"%accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0}))\n",
    "# load best model to test\n",
    "saver.restore(sess, best_model_path)\n",
    "print(\"test accuracy on best model %g %s\"% (accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0}), best_model_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
